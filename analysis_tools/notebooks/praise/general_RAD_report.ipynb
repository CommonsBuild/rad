{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f4ceb",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dist_notebook_path = \"\"\n",
    "input_files= \"\"\n",
    "#WARNING: When re-running the notebook for audit, change the dist_notebook_path below to \"./output_praiseDistribution.ipynb\"\n",
    "#then go to \"Cell > Run all\" -- This only works for the notebook in \n",
    "#\"distribution_results/round ?/results/analysis_outputs/output_general_RAD_report.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-unemployment",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "#this is mainly for when we re-run the notebook (TO DO: redo when file structure finalized)\n",
    "dir2 = os.path.abspath('../../../')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: sys.path.append(dir1)\n",
    "    \n",
    "\n",
    "from analysis_tools.module_libraries import general_tool_module as tools\n",
    "from analysis_tools.module_libraries import praise_tool_module as praise_tools\n",
    "\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "import scrapbook as sb\n",
    "\n",
    "#fc_praise = FileChooser('./')\n",
    "#fc_sourcecred = FileChooser('./')\n",
    "#fc_rewardboard = FileChooser('./')\n",
    "\n",
    "#print(\"== Please choose the Praise CSV file == \")\n",
    "#display(fc_praise)\n",
    "#print(\"== Please choose the Sourcecred CSV file == \")\n",
    "#display(fc_sourcecred)\n",
    "#print(\"== Please choose the Rewardboard address list CSV file == \")\n",
    "#display(fc_rewardboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f5dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the data from the previous notebook\n",
    "\n",
    "nb = sb.read_notebook(dist_notebook_path)\n",
    "\n",
    "CROSS_PERIOD_ROOT = input_files[\"cross_period_root\"]\n",
    "\n",
    "final_token_allocations = nb.scraps['final_token_allocations'].data\n",
    "rewardboard_rewards = nb.scraps['rewardboard_rewards'].data\n",
    "quantifier_rewards = nb.scraps['quantifier_rewards'].data\n",
    "quantifier_rating_table = nb.scraps['quantifier_rating_table'].data\n",
    "\n",
    "processed_praise = nb.scraps['processed_praise'].data\n",
    "praise_by_user = nb.scraps['praise_by_user'].data\n",
    "#processed_sourcecred = nb.scraps['processed_sourcecred'].data\n",
    "\n",
    "#sourcecred_distribution = nb.scraps['sourcecred_distribution'].data\n",
    "praise_distribution = nb.scraps['praise_distribution'].data\n",
    "\n",
    "DISTRIBUTION_NAME = nb.scraps['distribution_name'].data\n",
    "TOTAL_TOKENS_ALLOCATED = nb.scraps['total_tokens_allocated'].data\n",
    "DUPLICATE_PRAISE_WEIGHT = nb.scraps['praise_quantify_duplicate_praise_valuation'].data\n",
    "NUMBER_OF_QUANTIFIERS_PER_PRAISE = nb.scraps['quantifiers_per_praise'].data\n",
    "PERIOD_START_DATE = praise_distribution['DATE'].min()[:10]\n",
    "PERIOD_END_DATE = praise_distribution['DATE'].max()[:10]\n",
    "PSEUDONYMS_USED = nb.scraps['pseudonyms_used'].data\n",
    "\n",
    "pseudonym_string = \"were\"  if bool(PSEUDONYMS_USED) else \"were not\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-sussex",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh', logo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-image",
   "metadata": {},
   "source": [
    "# Rewards Analytics and Distribution Dashboard for Quantification Review\n",
    "This goal of this document is to offer an easy way to process the outputs of the praise reward system and perform an analysis of the resulting token reward distribution. It should be considered a work-in-progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "md(f\"<h2>Distribution report for {DISTRIBUTION_NAME}</h2> \\\n",
    "<ul><li>This period covers praise given between <b>{PERIOD_START_DATE}</b> and  <b>{PERIOD_END_DATE}</b>. </li> \\\n",
    "<li> We allocated a total of <b>{TOTAL_TOKENS_ALLOCATED}</b> TEC tokens for rewards. </li>\\\n",
    "<li>Duplicate praise received a weighting of <b>{DUPLICATE_PRAISE_WEIGHT}</b> the value of the original praise. </li> \\\n",
    "<li>We assigned <b>{NUMBER_OF_QUANTIFIERS_PER_PRAISE}</b> quantifiers per praise instance. </li> \\\n",
    "<li>Praise receiver names <b>{pseudonym_string}</b> hidden behind pseudonyms during quantification </li> \\\n",
    "</ul>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-provider",
   "metadata": {},
   "source": [
    "# Praise Data Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-peace",
   "metadata": {},
   "source": [
    "### Rating distribution\n",
    "Since praise gets valued on a scale, we can take a look at how often each value of the scale gets assigned by quantifiers.\n",
    "Note: how to process the duplicate scores? For now, just delete them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out the quantifiers who didn't give any rating (i.e. all scores are 0)\n",
    "quantifier_sum = quantifier_rating_table[['QUANT_ID','QUANT_VALUE']].groupby('QUANT_ID').sum()\n",
    "norating_quantifiers = quantifier_sum.loc[quantifier_sum['QUANT_VALUE']==0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-services",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = quantifier_rating_table[['QUANT_VALUE']].value_counts().rename_axis('QUANT_VALUE').reset_index(name='counts').sort_values(by=['QUANT_VALUE'])\n",
    "freq['QUANT_VALUE'] = freq['QUANT_VALUE'].astype('string')\n",
    "\n",
    "fig_freq = px.bar(freq, x=\"QUANT_VALUE\", y=\"counts\", labels={\"QUANT_VALUE\": \"Rating\",\"counts\": \"Number of appearances\"}, title=\"Praise Rating Distribution\", width=800, height=300)\n",
    "fig_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f4e252",
   "metadata": {},
   "source": [
    "### Praise Reward Distribution\n",
    "\n",
    "We can now take a look at the distribution of the received praise rewards. You can toggle the inclusion of the different sources by clicking on the legend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-computer",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#pr_distribution = praise_by_user[['USER IDENTITY', 'PERCENTAGE']].sort_values(by=['PERCENTAGE'], ascending=False)\n",
    "\n",
    "#fig_pr_distribution = px.bar(pr_distribution, x='USER IDENTITY', y='PERCENTAGE', labels={\"IDENTITY\": \"User\",\"PERCENTAGE\": \"% of total\"}, title=\"Praise Reward Distribution\")#.opts(width=800, height=500, title='SourceCred Distribution', xlabel='Value', ylabel='% of Total', xaxis='bare')\n",
    "#fig_pr_distribution.update_xaxes(showticklabels=False)\n",
    "\n",
    "#fig_pr_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: show usernames on the x axis\n",
    "#fig_final_alloc = px.bar(final_token_allocations, x=\"USER IDENTITY\", y = [\"QUANT_REWARD\", \"REWARDBOARD_REWARD\", \"PRAISE_REWARD\", \"SOURCECRED_REWARD\"], title=\"Rewards received by category\", color_discrete_map = {'PRAISE_REWARD': 'blue', 'SOURCECRED_REWARD': 'red', 'QUANT_REWARD':'green', 'REWARDBOARD_REWARD':'yellow'})\n",
    "fig_final_alloc = px.bar(final_token_allocations, x=\"USER IDENTITY\", y = [\"QUANT_REWARD\", \"REWARDBOARD_REWARD\", \"PRAISE_REWARD\"], title=\"Rewards received by category\", color_discrete_map = {'PRAISE_REWARD': 'blue', 'QUANT_REWARD':'green', 'REWARDBOARD_REWARD':'yellow'})\n",
    "fig_final_alloc.update_xaxes(showticklabels=False)\n",
    "fig_final_alloc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56c0c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "modnames = [\n",
    "  \"sources.praise.analysis.modules.giverTotalScore\", \n",
    "  \"sources.praise.analysis.modules.giverTotalScore2\"\n",
    "]\n",
    "modlist = []\n",
    "for lib in modnames:\n",
    "  mod = importlib.import_module(lib)\n",
    "  print(mod.header)\n",
    "  print(mod.description)\n",
    "  mod.run(praise_distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-drain",
   "metadata": {},
   "source": [
    "### Praise Flows\n",
    "\n",
    "Now for something more fun: let's surface the top \"praise flows\" from the data. Thanks to @inventandchill for this awesome visualization! \n",
    "On one side we have the top 20 praise givers separately (modifiable by changing the variable n_senders), on the other the top 25 receivers (modifiable by changing the variable n_receivers). The people outside the selection get aggregated into the \"REST FROM\" and \"REST TO\" categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-cleveland",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_SENDERS_FLOW = 15 #The left side, the praise senders. X largest ones + one bucket for the rest \n",
    "NUMBER_OF_RECEIVERS_FLOW = 25 #The right side, the praise receivers. X largest ones + one bucket for the rest \n",
    "dist_for_praise_flow = praise_distribution.rename(columns = {'FROM USER ACCOUNT':'FROM', 'TO USER ACCOUNT':'TO'})\n",
    "praise_flow = praise_tools.prepare_praise_flow(dist_for_praise_flow.copy(), n_senders=NUMBER_OF_SENDERS_FLOW, n_receivers=NUMBER_OF_RECEIVERS_FLOW)\n",
    "#praise_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-veteran",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%opts Sankey (cmap='Category10' edge_color='FROM' edge_line_width=0 node_alpha=1.0)\n",
    "%%opts Sankey [node_sort=False label_position='outer' bgcolor=\"snow\" node_width=40 node_sort=True ]\n",
    "%%opts Sankey [width=1000 height=800 title=\"Praise flow for Batch 1. Sum of Praise. Left - praise sender. Right - praise receiver\"]\n",
    "%%opts Sankey [margin=0 padding=0 show_values=True]\n",
    "\n",
    "hv.Sankey(praise_flow, kdims=[\"FROM\", \"TO\"], vdims=[\"AVG SCORE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8750ea",
   "metadata": {},
   "source": [
    "### Praise Reward Reception Change History\n",
    "See if we have more people giving and receiving praise now, as well as the total rating.\n",
    " TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7017899",
   "metadata": {
    "papermill": {
     "duration": 0.209823,
     "end_time": "2022-04-13T21:20:13.465387",
     "exception": false,
     "start_time": "2022-04-13T21:20:13.255564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Quantifier Data\n",
    "Let's take a closer look at the quantification process and and see if we can spot any problems:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f5f42",
   "metadata": {
    "papermill": {
     "duration": 0.210179,
     "end_time": "2022-04-13T21:20:13.881434",
     "exception": false,
     "start_time": "2022-04-13T21:20:13.671255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Praise Outliers\n",
    "To aid the revision process, we highlight disagreements between quantifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1ab13",
   "metadata": {},
   "source": [
    "### Outliers sort by spreads\n",
    "\n",
    "Here we generate a table which sorts the praise by the size of the spread between the highest and lowest scores. It gives us an overview of the spread distribution. <br>\n",
    "\n",
    "For an exhaustive list, take a look at the exported file \"praise_outliers.csv\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8f891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T21:20:14.299694Z",
     "iopub.status.busy": "2022-04-13T21:20:14.299471Z",
     "iopub.status.idle": "2022-04-13T21:20:14.318706Z",
     "shell.execute_reply": "2022-04-13T21:20:14.318142Z"
    },
    "papermill": {
     "duration": 0.233743,
     "end_time": "2022-04-13T21:20:14.320449",
     "exception": false,
     "start_time": "2022-04-13T21:20:14.086706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#for general use\n",
    "col_dismissed = [f'DISMISSED {k+1}' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "col_dupids = [f'DUPLICATE ID {k+1}' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "col_scores= [f'SCORE {k+1}' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "\n",
    "#clean the Dataframe and remove praise where there is dismissal agreement\n",
    "praisecheck_df = praise_distribution.drop(['TO ETH ADDRESS', 'TO USER ACCOUNT ID', 'FROM ETH ADDRESS', 'FROM USER ACCOUNT ID', 'SOURCE ID', 'SOURCE NAME', 'PERCENTAGE', 'TOKEN TO RECEIVE'], axis=1)\n",
    "ethadds = [f'QUANTIFIER {k+1} ETH ADDRESS' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "praisecheck_df.drop(ethadds,axis=1,inplace=True)\n",
    "\n",
    "praisecheck_clean_controversial = praisecheck_df.loc[praisecheck_df[col_dismissed].sum(axis=1)<NUMBER_OF_QUANTIFIERS_PER_PRAISE,:]\n",
    "\n",
    "\n",
    "#scores = ['SCORE '+str(k+1) for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "#praisecheck_df['SPREAD'] = praisecheck_df[scores].max(axis=1) - praisecheck_df[scores].min(axis=1)\n",
    "#sort_by_controversial = praisecheck_df.sort_values(by= 'SPREAD', ascending = False).reset_index()\n",
    "\n",
    "# Saves the outlier table in an external file for easier review\n",
    "#praise_outliers_csv = sort_by_controversial.to_csv(index=False)\n",
    "#with open('praise_outliers.csv', 'w') as f:\n",
    "#    f.write(praise_outliers_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f388e768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'praisecheck_clean_controversial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dupclean_praisecheck \u001b[38;5;241m=\u001b[39m \u001b[43mpraisecheck_clean_controversial\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m praisecheck_clean_controversial\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      3\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m j, dup_id_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(col_dupids):\n\u001b[1;32m      4\u001b[0m \t\t\u001b[38;5;66;03m#print(f'Label: {dup_id_label} + Row: {row[dup_id_label]} + \"NaN: {np.nan}')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'praisecheck_clean_controversial' is not defined"
     ]
    }
   ],
   "source": [
    "#reinstate the original scores for the duplicates before calculating the spread\n",
    "dupclean_praisecheck = praisecheck_clean_controversial.copy()\n",
    "for i, row in praisecheck_clean_controversial.iterrows():\n",
    "    for j, dup_id_label in enumerate(col_dupids):\n",
    "        #print(f'Label: {dup_id_label} + Row: {row[dup_id_label]} + \"NaN: {np.nan}')\n",
    "        if row[dup_id_label] is not None:\n",
    "            #find the score   \t   \n",
    "            find_value = praisecheck_df.loc[praisecheck_df['ID']==row[dup_id_label], col_scores[j]]\n",
    "\n",
    "            #FOR DEBUG:  \n",
    "            try:\n",
    "                #substitute it in dupclean \n",
    "                dupclean_praisecheck.at[i, str(col_scores[j])] = int(find_value)  \n",
    "            except:\n",
    "                #account for the bug in early rounds\n",
    "                dupclean_praisecheck.at[i, str(col_scores[j])] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad585df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discard no-shows (score = 0 and not dismissed, after the above check for duplicates) and calculate spread\n",
    "def noShow(a,b):\n",
    "    if int(a)==0 and bool(b)==False: \n",
    "        return np.nan\n",
    "    else:\n",
    "        return a\n",
    "\n",
    "\n",
    "for i,  score_col in enumerate(col_scores):\n",
    "    dupclean_praisecheck[score_col] = dupclean_praisecheck.apply(lambda x: noShow(x[score_col], x[col_dismissed[i]]) , axis=1) \n",
    "\n",
    "    \n",
    "dupclean_praisecheck['SPREAD'] = dupclean_praisecheck[col_scores].max(axis=1) - dupclean_praisecheck[col_scores].min(axis=1)\n",
    "sort_by_controversial = dupclean_praisecheck.sort_values(by= 'SPREAD', ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outlier table in an external file for easier review\n",
    "praise_outliers_csv = sort_by_controversial.to_csv(index=False)\n",
    "with open('praise_outliers.csv', 'w') as f:\n",
    "    f.write(praise_outliers_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers if there's dimissal \n",
    "#col_dismissed = [f'DISMISSED {k+1}' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "\n",
    "#clean_controversial = sort_by_controversial.loc[sort_by_controversial[col_dismissed].sum(axis=1)==0,:]\n",
    "\n",
    "# remove rows when there's duplication -- to be done, better deal with duplication\n",
    "#col_dupids = [f'DUPLICATE ID {k+1}' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "#col_scores= [f'SCORE {k+1}' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "\n",
    "#dupclean_controversial = clean_controversial.loc[[set(row[col_dupids].values)=={None} for kr,row in clean_controversial.iterrows()]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp: instead of remove duplications, use the original score to replace the duplicated score.\n",
    "# but there seems to be some problem in the current datasheet that this thing doesn't work right...\n",
    "#col_dupids = [f'DUPLICATE ID {k+1}' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "#col_quants= [f'QUANTIFIER {k+1} USERNAME' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "#col_scores= [f'SCORE {k+1}' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "\n",
    "#dupclean_controversial = clean_controversial.copy()\n",
    "#for kr, row in clean_controversial.iterrows():\n",
    "#    for kq,kcol_dup in enumerate(col_dupids):\n",
    "#        if row[kcol_dup] is not None:\n",
    "#            # find the score of the original \n",
    "#            duprow = sort_by_controversial.loc[sort_by_controversial['ID']==row[kcol_dup]]\n",
    "#            row_quant_id = duprow[col_quants].values== row[col_quants[kq]]\n",
    "#            dupscore = duprow[np.array(col_scores)[row_quant_id[0]]]\n",
    "            \n",
    "            # copy that into the duplicated praise as the score\n",
    "#            dupclean_controversial.at[kr,col_scores[kq]]= dupscore.values[0][0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d589a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some columns for easier visual examination\n",
    "#simp_controversial = dupclean_controversial.drop(col_dupids+col_dismissed,axis='columns')\n",
    "#simp_controversial.to_csv('cleaned_praise_outliers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what kinda messages get higher spread \n",
    "sort_by_controversial['MAX SCORE'] = sort_by_controversial[col_scores].max(axis=1)\n",
    "spread_df = sort_by_controversial[['REASON','AVG SCORE','MAX SCORE','SPREAD']].sort_values(by='SPREAD')\n",
    "px.scatter(spread_df,y='MAX SCORE',x='SPREAD',hover_name='REASON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1099050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T21:20:15.294678Z",
     "iopub.status.busy": "2022-04-13T21:20:15.294479Z",
     "iopub.status.idle": "2022-04-13T21:20:15.312352Z",
     "shell.execute_reply": "2022-04-13T21:20:15.311885Z"
    },
    "papermill": {
     "duration": 0.246837,
     "end_time": "2022-04-13T21:20:15.313981",
     "exception": false,
     "start_time": "2022-04-13T21:20:15.067144",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sort_by_controversial.head(20) # top 20 spreads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174bc9de",
   "metadata": {},
   "source": [
    "This is a visual aid. ATTENTION! If there are several praise instances with similar spread and quant score, all but one end up \"hidden\" on the chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c81b1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T21:20:14.772859Z",
     "iopub.status.busy": "2022-04-13T21:20:14.772641Z",
     "iopub.status.idle": "2022-04-13T21:20:14.828749Z",
     "shell.execute_reply": "2022-04-13T21:20:14.828205Z"
    },
    "papermill": {
     "duration": 0.265641,
     "end_time": "2022-04-13T21:20:14.831205",
     "exception": false,
     "start_time": "2022-04-13T21:20:14.565564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fig_spread = px.scatter(sort_by_controversial, x=\"AVG SCORE\", y=\"SPREAD\", hover_data=[sort_by_controversial.index, 'ID'])\n",
    "#fig_spread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9cadd1",
   "metadata": {
    "papermill": {
     "duration": 0.215617,
     "end_time": "2022-04-13T21:20:15.767379",
     "exception": false,
     "start_time": "2022-04-13T21:20:15.551762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Praise score by quantifier -- outliers among the quantifiers?\n",
    "\n",
    "Let's see the range of praise scores every quantifier gave to see the behavior difference of quantifiers.\n",
    "\n",
    "To interpret the box plot:\n",
    "\n",
    "- Bottom horizontal line of box plot is minimum value\n",
    "\n",
    "- First horizontal line of rectangle shape of box plot is First quartile or 25%\n",
    "\n",
    "- Second horizontal line of rectangle shape of box plot is Second quartile or 50% or median.\n",
    "\n",
    "- Third horizontal line of rectangle shape of box plot is third quartile or 75%\n",
    "\n",
    "- Top horizontal line of rectangle shape of box plot is maximum value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17817789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T21:20:16.205233Z",
     "iopub.status.busy": "2022-04-13T21:20:16.205027Z",
     "iopub.status.idle": "2022-04-13T21:20:16.263571Z",
     "shell.execute_reply": "2022-04-13T21:20:16.263168Z"
    },
    "papermill": {
     "duration": 0.278811,
     "end_time": "2022-04-13T21:20:16.265747",
     "exception": false,
     "start_time": "2022-04-13T21:20:15.986936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_boxplot = quantifier_rating_table[['QUANT_ID', 'QUANT_VALUE']].copy()\n",
    "fig_box = px.box(quant_boxplot, x=\"QUANT_ID\", y=\"QUANT_VALUE\", points=False)\n",
    "fig_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc7886",
   "metadata": {},
   "source": [
    "## Agreement on duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process to remove None\n",
    "dup_agree_check = dupclean_praisecheck.copy()\n",
    "\n",
    "for k in range(1,NUMBER_OF_QUANTIFIERS_PER_PRAISE+1):\n",
    "    col = 'DUPLICATE ID '+str(k)\n",
    "    dup_agree_check.loc[dup_agree_check[col].isnull(),col]=0\n",
    "\n",
    "dup_agree_check['DUPLICATION AGREED'] = [len(set(kk))==1 for kk in dup_agree_check[col_dupids].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the duplication disagreement\n",
    "duplication_disagreement = dup_agree_check.loc[dup_agree_check['DUPLICATION AGREED']!=True,:]\n",
    "\n",
    "duplication_disagreement = duplication_disagreement.drop(['SPREAD']+col_scores+col_dismissed, axis=1)\n",
    "\n",
    "print(f'Among {len(dup_agree_check)} praises, {len(duplication_disagreement)} ({len(duplication_disagreement)/len(praisecheck_df)*100:.2f}%) do not agree on duplication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3e5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#col_dup = [f'DUPLICATE MSG {k+1}' for k in range(NUMBER_OF_QUANTIFIERS_PER_PRAISE)]\n",
    "#duplication_disagreement[col_dup] = '/'\n",
    "#unfound_dupid=[]\n",
    "#for kr,row in duplication_disagreement.iterrows():\n",
    "#    for k in range(1,NUMBER_OF_QUANTIFIERS_PER_PRAISE+1):\n",
    "#        dup_id=row['DUPLICATE ID '+str(k)]\n",
    "#        if dup_id!=0:\n",
    "#            try:\n",
    "#                duplication_disagreement['DUPLICATE MSG '+str(k)].at[kr] = praisecheck_df.loc[praisecheck_df['ID']==dup_id,'REASON'].values[0]\n",
    "#            except:\n",
    "#                if len(praisecheck_df.loc[praisecheck_df['ID']==dup_id,'REASON'].values)==0:\n",
    "#                    unfound_dupid.append(dup_id)\n",
    "#                    duplication_disagreement['DUPLICATE MSG '+str(k)].at[kr] = 'duplication not found'\n",
    "#if len(unfound_dupid)>0:\n",
    "#    print(f'{len(unfound_dupid)} duplication ids not found!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60212a2",
   "metadata": {},
   "source": [
    "Praise instances with disagreements in duplication are collected in 'results/duplication_examination.csv'. To compare, look at the last 4 columns: 'DUPLICATE MSG 1/2/3' and 'ORIGINAL MSG'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcaeb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplication_disagreement['ORIGINAL MSG']=duplication_disagreement['REASON'] # replicate this column just after the other messages for easy comparison\n",
    "\n",
    "duplication_disagreement.to_csv('duplication_examination.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac3793",
   "metadata": {},
   "source": [
    "## Agreement on dismissal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201537d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process to remove None\n",
    "dism_agree_check = dupclean_praisecheck.copy()\n",
    "\n",
    "for k in range(1,NUMBER_OF_QUANTIFIERS_PER_PRAISE+1):\n",
    "    col = 'DISMISSED '+str(k)\n",
    "    dism_agree_check.loc[dism_agree_check[col].isnull(),col]=0\n",
    "\n",
    "\n",
    "dism_agree_check['DISMISSAL AGREED'] = [len(set(kk))==1 for kk in dism_agree_check[col_dismissed].values]\n",
    "\n",
    "dismiss_disagreement = dism_agree_check.loc[dism_agree_check['DISMISSAL AGREED']==False,:]\n",
    "dismiss_disagreement= dismiss_disagreement.drop(col_scores+col_dupids+['SPREAD'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec07ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dismiss_disagreement['ORIGINAL MSG']=dismiss_disagreement['REASON'] # replicate this column just after the other messages for easy comparison\n",
    "\n",
    "dismiss_disagreement.to_csv('dismissal_disagreed.csv')\n",
    "\n",
    "print(f'Among {len(dism_agree_check)} praises, {len(dismiss_disagreement)} ({len(dismiss_disagreement)/len(praisecheck_df)*100:.2f}%) do not agree on dismissal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74295e31",
   "metadata": {},
   "source": [
    "Praise instances with disagreements in dismissal are collected in'results/dismissal_disaggreed.csv'. You can further look into who dismissed and who did not. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7fc054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.195px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
